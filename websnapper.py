# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11_weumxGPnJADZS7p_E5wpMQYq2Xs14L
"""

import requests
import bs4

data = requests.get("https://www.checkraka.com/house/?quicksearch_order=ASC&page=1")
data

soup = bs4.BeautifulSoup(data.text)

all_house = soup.find_all('div', {'class' : 'product-highlight-item category-44'})

for hourse in all_house:
    print(hourse.find('div', {'class' : 'mid'}).text.strip())
    print(hourse.find('div', {'class' : 'card'}).text)
    print

import requests
from bs4 import BeautifulSoup
import csv

# Function to fetch data from a single page
def fetch_page(page_number):
    url = f"https://www.checkraka.com/house/?quicksearch_order=ASC&page={page_number}"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    return soup

# Function to extract data from a page
def extract_data(soup):
    data = []
    all_house = soup.find_all("div", {"class": "product-highlight-item category-44"})
    for house in all_house:
        card = house.find('div', {'class': 'card'})
        if card:
            spans = card.find_all('span')
            labels = card.find_all('label')

            # Extract and clean 'name' from spans
            name_parts = [span.get_text(strip=True) for span in spans]
            name = ' '.join(name_parts).replace("เพิ่มเพื่อเปรียบเทียบ", "").strip()

            # Extract 'price' from labels
            price = ' '.join(label.get_text(strip=True) for label in labels)

            row = {
                'name': name,
                'price': price
            }
            data.append(row)
    return data

# Function to save data to CSV
def save_to_csv(data, filename):
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=['name', 'price'])
        writer.writeheader()
        for row in data:
            writer.writerow(row)

def main():
    all_data = []
    for page_number in range(1, 6):  # Fetching data from 5 pages
        print(f"Fetching page {page_number}...")
        soup = fetch_page(page_number)
        data = extract_data(soup)
        all_data.extend(data)

    save_to_csv(all_data, 'houses.csv')
    print("Data has been written to houses.csv")

if __name__ == "__main__":
    main()